{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Convolutional Neural Network (CNN) on CIFAR-10 Using TensorFlow (Transfer Learning)\n",
        "\n",
        "This project introduces foundational concepts of deep learning with TensorFlow by building, training, and evaluating a Convolutional Neural Network (CNN) on the CIFAR-10 dataset. The goal is to classify 32x32 RGB images into 10 distinct categories (e.g., airplanes, cars, birds). Participants will gain hands-on experience with TensorFlow workflows, CNN architectures, and image classification techniques.\n",
        "\n",
        "Model Performance: Achieve > 70% test accuracy (baseline)."
      ],
      "metadata": {
        "id": "ACXtDioeHw0C"
      },
      "id": "ACXtDioeHw0C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c16997d-c671-4bfb-a382-e46271ce665d",
      "metadata": {
        "id": "3c16997d-c671-4bfb-a382-e46271ce665d"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c1f7a2-7e58-4745-824f-a2f715d67cfc",
      "metadata": {
        "id": "79c1f7a2-7e58-4745-824f-a2f715d67cfc"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8221ff22-3cad-40c0-9493-d640ad2056e0",
      "metadata": {
        "id": "8221ff22-3cad-40c0-9493-d640ad2056e0"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3405118f-034b-4d44-8e6c-fe3152c2fc5c",
      "metadata": {
        "id": "3405118f-034b-4d44-8e6c-fe3152c2fc5c"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7072263c-659a-4b5f-be39-ac0419437ea0",
      "metadata": {
        "id": "7072263c-659a-4b5f-be39-ac0419437ea0"
      },
      "outputs": [],
      "source": [
        "# Split validation set from train images\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**\n",
        "\n",
        "Using ResNet50 model. For preprocessing, it is better to resize the image to the shape of the image, the base model was trained in. that's why I resized the 32x32 to 224x224. Also normalize it to the way resnet50 normalizes the images [1, -1]. In the first notebook, i normalized it to [0, 1] but when using a base model, i have learned that you should preprocess to make the images as close to the orginal images used to train the model, as possible."
      ],
      "metadata": {
        "id": "LSh8r0v3H5bE"
      },
      "id": "LSh8r0v3H5bE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4601a4f-8de4-42a7-9bb8-a80eca0b079e",
      "metadata": {
        "id": "a4601a4f-8de4-42a7-9bb8-a80eca0b079e"
      },
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "def preprocess_and_augment(image, target_size=(224, 224)):\n",
        "  # Resize the image to 224x224 (ResNet50 input size)\n",
        "  image = tf.image.resize(image, target_size)\n",
        "\n",
        "  # Normalize the image using ResNet50 preprocessing\n",
        "  image = keras.applications.resnet50.preprocess_input(image)\n",
        "\n",
        "  # Apply data augmentation\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.image.random_flip_up_down(image)\n",
        "  image = tf.image.rot90(image, k=tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32))\n",
        "  image = tf.image.random_contrast(image, lower=0.2, upper=0.5)\n",
        "\n",
        "  return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the preprocessing, I wasn't able to apply it all at once without getting \"LimitedResources\" error. So, I processed it in batches. To be able to do this, I changed the numpy datase to tensorflow dataset so that they can be processed in batches"
      ],
      "metadata": {
        "id": "R09MDIftI5OQ"
      },
      "id": "R09MDIftI5OQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f058fcd-5f06-4a20-9d80-d9cc6594b1ff",
      "metadata": {
        "id": "2f058fcd-5f06-4a20-9d80-d9cc6594b1ff"
      },
      "outputs": [],
      "source": [
        "# Create a dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "\n",
        "# Apply transformations\n",
        "train_dataset = train_dataset.map(lambda x, y: (preprocess_and_augment(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.map(lambda x, y: (preprocess_and_augment(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Batch the dataset and optimize performance with prefetch\n",
        "train_dataset = train_dataset.batch(batch_size=32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size=32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Model**"
      ],
      "metadata": {
        "id": "zF0_cIkbJQ9p"
      },
      "id": "zF0_cIkbJQ9p"
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.ResNet50(include_top=False, weights=\"imagenet\", pooling=\"avg\")\n",
        "\n",
        "base_model.trainable = False # Freeze the weights of the top layers first\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.InputLayer(shape=[224, 224, 3]),\n",
        "    base_model,\n",
        "    keras.layers.Dense(units=128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Dense(units=10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "xxEVcZYQ7RTb",
        "outputId": "871dfa73-6035-402b-a37c-82038d9bb429"
      },
      "id": "xxEVcZYQ7RTb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m262,272\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,274\u001b[0m (90.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,274</span> (90.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,562\u001b[0m (1.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,562</span> (1.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Train 1\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9WfrXB38Rka",
        "outputId": "3a40e386-cda4-40d5-ce55-c9442eef0102"
      },
      "id": "G9WfrXB38Rka",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 93ms/step - accuracy: 0.6223 - loss: 1.2527 - val_accuracy: 0.7058 - val_loss: 0.9450\n",
            "Epoch 2/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7271 - loss: 0.8876 - val_accuracy: 0.7184 - val_loss: 0.9164\n",
            "Epoch 3/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7398 - loss: 0.8504 - val_accuracy: 0.7174 - val_loss: 0.9060\n",
            "Epoch 4/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7488 - loss: 0.8250 - val_accuracy: 0.7244 - val_loss: 0.8914\n",
            "Epoch 5/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 88ms/step - accuracy: 0.7513 - loss: 0.8127 - val_accuracy: 0.7392 - val_loss: 0.8484\n",
            "Epoch 6/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7550 - loss: 0.8070 - val_accuracy: 0.7494 - val_loss: 0.8352\n",
            "Epoch 7/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7574 - loss: 0.8016 - val_accuracy: 0.7370 - val_loss: 0.8634\n",
            "Epoch 8/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7617 - loss: 0.7913 - val_accuracy: 0.7444 - val_loss: 0.8554\n",
            "Epoch 9/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7653 - loss: 0.7896 - val_accuracy: 0.7370 - val_loss: 0.8789\n",
            "Epoch 10/10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 87ms/step - accuracy: 0.7660 - loss: 0.7809 - val_accuracy: 0.7420 - val_loss: 0.8540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.layers[-10].trainable = True # Unfreeze the last 10 layers to finetune\n",
        "\n",
        "# ReCompile the model\n",
        "model.compile(optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Set up callbacks\n",
        "my_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "    keras.callbacks.ModelCheckpoint(\"cifar10_tl.keras\", save_best_only=True),\n",
        "]\n",
        "\n",
        "# Train 2\n",
        "history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=my_callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-BiMRRTEVXv",
        "outputId": "e1950b43-0fb1-4158-c67a-2206805f8a64"
      },
      "id": "O-BiMRRTEVXv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 96ms/step - accuracy: 0.7831 - loss: 0.7208 - val_accuracy: 0.7538 - val_loss: 0.8238\n",
            "Epoch 2/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7798 - loss: 0.7298 - val_accuracy: 0.7486 - val_loss: 0.8414\n",
            "Epoch 3/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 90ms/step - accuracy: 0.7818 - loss: 0.7390 - val_accuracy: 0.7564 - val_loss: 0.8222\n",
            "Epoch 4/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7817 - loss: 0.7366 - val_accuracy: 0.7572 - val_loss: 0.8298\n",
            "Epoch 5/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7841 - loss: 0.7331 - val_accuracy: 0.7464 - val_loss: 0.8498\n",
            "Epoch 6/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7856 - loss: 0.7300 - val_accuracy: 0.7552 - val_loss: 0.8445\n",
            "Epoch 7/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7885 - loss: 0.7203 - val_accuracy: 0.7542 - val_loss: 0.8363\n",
            "Epoch 8/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7895 - loss: 0.7217 - val_accuracy: 0.7568 - val_loss: 0.8421\n",
            "Epoch 9/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7900 - loss: 0.7162 - val_accuracy: 0.7584 - val_loss: 0.8333\n",
            "Epoch 10/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7916 - loss: 0.7160 - val_accuracy: 0.7498 - val_loss: 0.8588\n",
            "Epoch 11/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7921 - loss: 0.7135 - val_accuracy: 0.7608 - val_loss: 0.8232\n",
            "Epoch 12/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7940 - loss: 0.7090 - val_accuracy: 0.7502 - val_loss: 0.8535\n",
            "Epoch 13/100\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.7935 - loss: 0.7106 - val_accuracy: 0.7548 - val_loss: 0.8468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Accuracy: 79%\n",
        "Val Accuracy: 75%\n",
        "\n",
        "It doesn't seem to be overfitting. Slight increase from my custom model, although I wish it could be more. Let's evaluate on the test model\n",
        "\n",
        "**Evaluation**\n",
        "\n",
        "First, preprocess the model to look like the resnet expected input size and normalize it too."
      ],
      "metadata": {
        "id": "Ekrl2kTPJphW"
      },
      "id": "Ekrl2kTPJphW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess\n",
        "def preprocess(image, target_size=(224, 224)):\n",
        "  # Resize the image to 224x224 (ResNet50 input size)\n",
        "  image = tf.image.resize(image, target_size)\n",
        "\n",
        "  # Normalize the image using ResNet50 preprocessing\n",
        "  image = keras.applications.resnet50.preprocess_input(image)\n",
        "\n",
        "  return image\n",
        "\n",
        "# Create a dataset for the test set\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "test_dataset = test_dataset.map(lambda x, y: (preprocess(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.batch(batch_size=32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "mvEbqSFviLKf"
      },
      "id": "mvEbqSFviLKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now evaluate on the test model\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0EJSTi9rT19",
        "outputId": "8a63b3c0-0693-44a9-edcd-e8a0959a0d37"
      },
      "id": "w0EJSTi9rT19",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8124 - loss: 0.6832\n",
            "Test Loss: 0.6791001558303833, Test Accuracy: 0.8130000233650208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vi24QBkBK7PY"
      },
      "id": "vi24QBkBK7PY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Accuracy: 81%\n",
        "\n",
        "This is much better than 69% of my custom model. Which is good."
      ],
      "metadata": {
        "id": "G3_UdZytKW_g"
      },
      "id": "G3_UdZytKW_g"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}